{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 지정된 경로를 찾을 수 없습니다: 'D:\\\\repos\\\\data\\\\2016 PHM Data Challenge\\\\2016 PHM DATA CHALLENGE CMP DATA SET\\\\CMP-data\\\\training'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ONEPRE~1\\AppData\\Local\\Temp/ipykernel_33552/2862112118.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mstage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"training\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[0mstage_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'CMP-data\\\\'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Processed data set/train_data.npy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONEPRE~1\\AppData\\Local\\Temp/ipykernel_33552/2862112118.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m(PATH, stage, stage_x)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m     \u001b[0mdataframe_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabstract_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m         \u001b[1;31m# WAFER_ID 및 STAGE 데이터 삭제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ONEPRE~1\\AppData\\Local\\Temp/ipykernel_33552/2862112118.py\u001b[0m in \u001b[0;36mload_dataframe\u001b[1;34m(PATH, stage, stage_x)\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstage_x\u001b[0m\u001b[1;33m)\u001b[0m                                     \u001b[1;31m#경로 이름이 연결되고 파일 경로가 획득됩니다. PATH\\stage_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdataframe_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mdataframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TIMESTAMP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m                  \u001b[1;31m#'TIMESTAMP'데이터는 의미 없는 평균 데이터, 삭제\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 지정된 경로를 찾을 수 없습니다: 'D:\\\\repos\\\\data\\\\2016 PHM Data Challenge\\\\2016 PHM DATA CHALLENGE CMP DATA SET\\\\CMP-data\\\\training'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# '''\n",
    "PATH= \"C:\\\\Users\\\\OnePredict\\\\Desktop\\\\CMP\\\\2016 PHM DATA CHALLENGE CMP DATA SET\\\\\"\n",
    "stage = \"test\"\n",
    "stage_x = 'CMP-data\\\\'+stage\n",
    "# '''\n",
    "def load_dataframe(PATH, stage, stage_x):\n",
    "    # get x and y from corresponding dirs\n",
    "    path = os.path.join(PATH, stage_x)                                     #경로 이름이 연결되고 파일 경로가 획득됩니다. PATH\\stage_x\n",
    "    dataframe_x = pd.DataFrame()\n",
    "    for file_name in os.listdir(path):\n",
    "        dataframe = pd.read_csv(os.path.join(path, file_name))\n",
    "        dataframe = dataframe.drop(columns=['TIMESTAMP'])                  #'TIMESTAMP'데이터는 의미 없는 평균 데이터, 삭제\n",
    "        dataframe_x = dataframe_x.append(dataframe,ignore_index=True)\n",
    "    # dataframe_group_x = dataframe_x.groupby(['WAFER_ID','STAGE'])\n",
    "    y_path = os.path.join(PATH, \"CMP-\"+stage+\"-removalrate.csv\")           #CMP-test-removalrate.csv\n",
    "    dataframe_y = pd.read_csv(y_path)\n",
    "\n",
    "    dataframe_y = dataframe_y.loc[dataframe_y['AVG_REMOVAL_RATE'] <= 1000] #1000보다 큰 데이터는 이상값으로 간주되어 폐기될 수 있습니다.\n",
    "    dataframe_y.hist('AVG_REMOVAL_RATE')                                   #이 열에 대한 분포 히스토그램을 그립니다.\n",
    "    # plt.hist(dataframe_y['AVG_REMOVAL_RATE'])                            #이 데이터 열을 플로팅합니다. 이 데이터 열은 두 섹션으로 명확하게 나뉩니다.\n",
    "    # plt.show()\n",
    "\n",
    "    # print(\"dataframe_x.shape\",dataframe_x.shape)\n",
    "    # print(\"dataframe_y.shape\", dataframe_y.shape)\n",
    "    return dataframe_x, dataframe_y\n",
    "\n",
    "\n",
    "def abstract_statistics(dataframe_x, dataframe_y, statistics=['mean','std','min','median','max']):\n",
    "    # abstract statistics for virtual metrology\n",
    "    # dataframe_x has dropped timestamps\n",
    "    dataframe_group_x = dataframe_x.groupby(['WAFER_ID','STAGE'])                       # 그룹 데이터\n",
    "\n",
    "    dataframe_statistics = dataframe_group_x.agg(statistics)                            # 그룹핑 후 각 차원 데이터의 관련 통계 계산\n",
    "    # print(\"dataframe_statistics\",dataframe_statistics)\n",
    "    # dataframe_statistics.to_csv(\"dataframe_statistics.csv\", index=False, sep=',')     # 위의 관련 통계를 csv 파일에 씁니다.\n",
    "\n",
    "    columns = dataframe_x.columns                                                       # 원본 데이터의 열 이름\n",
    "    dataframe_statistics.columns = generate_columns_name(columns, statistics)           # 위의 관련 통계 데이터에 대한 새 열 이름 생성\n",
    "    dataframe_statistics = pd.DataFrame(dataframe_statistics)\n",
    "    dataframe_statistics.reset_index(inplace=True)                                      # 인덱스 복원 및 누락된 데이터 채우기\n",
    "    # dataframe_statistics.to_csv(\"dataframe_statistics_final.csv\", index=False, sep=',') # 위의 관련 통계를 csv 파일에 씁니다.\n",
    "\n",
    "    # data = pd.concat([dataframe_statistics, dataframe_y], ignore_index=True)\n",
    "    data = pd.merge(dataframe_statistics, dataframe_y)\n",
    "    # data.to_csv(\"data_final.csv\", index=False, sep=',')\n",
    "    return data\n",
    "\n",
    "\n",
    "#함수 기능: 관련 통계 데이터에 대한 새 열 이름 생성\n",
    "#입력 매개변수: 열: 특성 데이터, 통계: 특성 데이터에 대해 계산할 통계\n",
    "def generate_columns_name(columns, statistics):\n",
    "    columns_list = []\n",
    "    for column in columns:\n",
    "        for statistic in statistics:\n",
    "            if column not in ['MACHINE_ID','MACHINE_DATA','TIMESTAMP','WAFER_ID','STAGE']:  #dataframe_statistics 테이블에 없는 열 이름 필터링\n",
    "                columns_list.append(statistic + \"_\" + column)\n",
    "    return columns_list\n",
    "\n",
    "def load_data(PATH, stage, stage_x):\n",
    "    dataframe_x, dataframe_y = load_dataframe(PATH, stage, stage_x)\n",
    "    train_data = abstract_statistics(dataframe_x, dataframe_y)\n",
    "    train_data = train_data[train_data.columns[2:]].values         # WAFER_ID 및 STAGE 데이터 삭제\n",
    "    return train_data\n",
    "\n",
    "\n",
    "#함수 기능：모드 분할\n",
    "def split_data(data, partitions=[50,100,165]):\n",
    "    n = len(partitions)\n",
    "    start = partitions[0]\n",
    "    splited_data = []\n",
    "    idx = np.where(data[:,-1]<=start)\n",
    "    splited_data.append(np.squeeze(data[idx,:],axis=0))\n",
    "    for i in range(1,n):\n",
    "        end = partitions[i]\n",
    "        idx = np.where(data[:,-1]<=start)\n",
    "        splited_data.append(np.squeeze(data[idx,:], axis=0))\n",
    "        start = end\n",
    "    idx = np.where(data[:,-1]>start)\n",
    "    splited_data.append(np.squeeze(data[idx,:],axis=0))\n",
    "    # print(splited_data)\n",
    "    return splited_data\n",
    "\n",
    "def split_data_label(data):\n",
    "    x = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    return x, y\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 훈련 데이터 세트, 테스트 데이터 세트 및 검증 데이터 세트를 별도로 생성\n",
    "    # train_data\n",
    "    PATH = \"D:\\\\repos\\\\data\\\\2016 PHM Data Challenge\\\\2016 PHM DATA CHALLENGE CMP DATA SET\\\\\"\n",
    "    stage = \"training\"\n",
    "    stage_x = 'CMP-data\\\\'+stage\n",
    "    train_data = load_data(PATH, stage, stage_x)\n",
    "    np.save(\"./Processed data set/train_data.npy\", train_data)\n",
    "\n",
    "    # test_data\n",
    "    PATH = \"D:\\\\repos\\\\data\\\\2016 PHM Data Challenge\\\\2016 PHM DATA CHALLENGE CMP DATA SET\\\\\"\n",
    "    stage = \"test\"\n",
    "    stage_x = 'CMP-data\\\\'+stage\n",
    "    test_data =load_data(PATH, stage, stage_x)\n",
    "    np.save(\"./Processed data set/test_data.npy\", test_data)\n",
    "\n",
    "    # validation_data\n",
    "    PATH = \"D:\\\\repos\\\\data\\\\2016 PHM Data Challenge\\\\2016 PHM DATA CHALLENGE CMP VALIDATION DATA SET\\\\\"\n",
    "    stage = \"validation\"\n",
    "    stage_x = stage\n",
    "    validation_data = load_data(PATH, stage, stage_x)\n",
    "    np.save(\"./Processed data set/validation_data.npy\", validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import data_utils\n",
    "\n",
    "\n",
    "def try_different_model(regressor, X_train, X_test, y_train, y_test):\n",
    "    modleName = str(regressor).split(\"(\")  # 모델 이름 작성 \n",
    "    modleName = modleName[0]\n",
    "\n",
    "    regressor.fit(X_train, y_train)\n",
    "    expected = y_test\n",
    "    predicted = regressor.predict(X_test)\n",
    "\n",
    "    # 예측값 이미지와 실제값 이미지 그리기\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    # plt.scatter(np.arange(len(y_test)), expected, s=2, c='b', marker='.', label='true value')\n",
    "    plt.plot(np.arange(len(y_test)), expected, color='blue', linewidth=1.0, marker='.', linestyle='-',\n",
    "             label='true value')\n",
    "    plt.plot(np.arange(len(y_test)), predicted, color='red', linewidth=1.0, marker='*', linestyle='-',\n",
    "             label='predict value')\n",
    "    plt.title(modleName + \"  \" + 'prediction curve')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(modleName + \".png\")  # 모델 이미지 저장\n",
    "    plt.show()\n",
    "    # 예측의 정확성 평가\n",
    "    print('MSE: ', mean_squared_error(expected, predicted))  # 평균 제곱 오차는 작을수록 좋습니다.\n",
    "    print('RMSE: ', np.sqrt(mean_squared_error(expected, predicted)))  # 제곱 평균 제곱근 오차는 작을수록 좋습니다.\n",
    "    print('MAE: ', mean_absolute_error(expected, predicted))  # 평균 절대 오차는 작을수록 좋습니다.\n",
    "    print('R^2: ', r2_score(expected, predicted))  # r2 score 전체 점수는 1이며 1에 가까울수록 좋습니다.\n",
    "\n",
    "\n",
    "##############  load train and test   #############\n",
    "test_data = np.load(\"./Processed data set/test_data.npy\")\n",
    "train_data = np.load(\"./Processed data set/train_data.npy\")\n",
    "validation_data = np.load(\"./Processed data set/validation_data.npy\")\n",
    "\n",
    "########################### regressors config ###########################\n",
    "# SVR\n",
    "SVR = svm.SVR()\n",
    "\n",
    "#SVR의 다양한 매개변수로 실험\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 각 모델을 차례로 호출하기 쉽도록 위의 모델을 결합하십시오.\n",
    "models = [SVR]\n",
    "\n",
    "partitions = [120]\n",
    "splited_train_data = data_utils.split_data(train_data, partitions)\n",
    "splited_test_data = data_utils.split_data(test_data, partitions)\n",
    "num_partitions = len(partitions) + 1\n",
    "\n",
    "for siglemodel in models:                        # 각 분류 모델을 차례로 호출\n",
    "    modleName = str(siglemodel).split(\"(\")       # 현재 예측 모델의 이름\n",
    "    print(\"현재 사용중인 모델\" + \"  \" + modleName[0])\n",
    "    for num in range(num_partitions):            # 데이터 세분화\n",
    "        print('set{}'.format(num))\n",
    "        train_data = splited_train_data[num]\n",
    "        X_train, y_train = data_utils.split_data_label(train_data)\n",
    "        test_data = splited_test_data[num]\n",
    "        X_test, y_test = data_utils.split_data_label(test_data)\n",
    "        ss = StandardScaler()                    # 평균과 분산을 정규화하는 Sklearn 라이브러리 함수\n",
    "        X_train = ss.fit_transform(X_train)\n",
    "        X_test = ss.transform(X_test)\n",
    "        try_different_model(siglemodel, X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
